"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.startStream = void 0;
const client_s3_1 = require("@aws-sdk/client-s3");
const s3fetchers_1 = require("./s3fetchers");
const utils_1 = require("./utils");
function startStream(config, onStreamerMessageReceived) {
    return __awaiter(this, void 0, void 0, function* () {
        const s3Client = new client_s3_1.S3Client({ region: config.s3RegionName });
        let lastProcessedBlockHash;
        let startFromBlockHeight = config.startBlockHeight;
        let queue = [];
        while (true) {
            let blockHeights;
            try {
                blockHeights = yield (0, s3fetchers_1.listBlocks)(s3Client, config.s3BucketName, startFromBlockHeight);
            }
            catch (err) {
                console.error("Failed to list blocks. Retrying.", err);
                continue;
            }
            if (blockHeights.length === 0) {
                yield (0, utils_1.sleep)(2000);
                continue;
            }
            for (let blockHeight of blockHeights) {
                const streamerMessage = yield (0, s3fetchers_1.fetchStreamerMessage)(s3Client, config.s3BucketName, blockHeight);
                // check if we have `lastProcessedBlockHash` (might be not set only on start)
                // compare lastProcessedBlockHash` with `streamerMessage.block.header.prevHash` of the current
                // block (ensure we never skip blocks even if there is some incident on Lake Indexer side)
                // retrieve the data from S3 if hashes don't match and repeat the main loop step
                if (lastProcessedBlockHash &&
                    lastProcessedBlockHash !== streamerMessage.block.header.prevHash) {
                    console.log("The hash of the last processed block doesn't match the prevHash of the new one. Refetching the data from S3 in 200ms", lastProcessedBlockHash, streamerMessage.block.header.prevHash);
                    yield (0, utils_1.sleep)(200);
                    break;
                }
                // `queue` here is used to achieve throttling as streamer would run ahead without a stop
                // and if we start from genesis it will spawn millions of `onStreamerMessageReceived` callbacks.
                // This implementation has a pipeline that fetches the data from S3 while `onStreamerMessageReceived`
                // is being processed, so even with a queue size of 1 there is already a benefit.
                queue.push(onStreamerMessageReceived(streamerMessage));
                if (queue.length > 10) {
                    yield queue.shift();
                }
                lastProcessedBlockHash = streamerMessage.block.header.hash;
                startFromBlockHeight = streamerMessage.block.header.height + 1;
            }
        }
    });
}
exports.startStream = startStream;
